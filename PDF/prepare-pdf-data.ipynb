{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#load our environment\n",
    "load_dotenv()\n",
    "\n",
    "ENV_NAM = os.getenv(\"ENV_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to get the number of tokens of a text string\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    token_integers = encoding.encode(string)\n",
    "    num_tokens = len(token_integers)\n",
    "\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to get the token length with the encoding\n",
    "\n",
    "tokenizer_name = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokenizer = tiktoken.get_encoding(tokenizer_name.name)\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(text, disallowed_special=())\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=8000,\n",
    "    chunk_overlap=20,\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads files from the pdf folder and creates a txt file from each pdf and puts it into the txt directory\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "def convert_pdf_to_txt(path, txt_dir):\n",
    "    fname = os.path.basename(path)[:-4]\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "        #define tow path for store the pdf to txt file\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    text = retstr.getvalue()\n",
    "    file_name_txt = fname+\".txt\"\n",
    "    txt_path = os.path.join(txt_dir, file_name_txt)\n",
    "    file = open(txt_path, \"w\", encoding=codec)\n",
    "    file.write(text)\n",
    "    file.close()\n",
    "    retstr.close()\n",
    "\n",
    "#path of the PDF file\n",
    "srcDir = \"./pdf/\"\n",
    "#path for saving the txt files\n",
    "txtDir = \"./txt/\"\n",
    "#scan files from Soruce directory\n",
    "for file in listdir(srcDir):\n",
    "    #check for PDF\n",
    "    if file.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(srcDir, file)\n",
    "        convert_pdf_to_txt(file_path, txtDir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanup the txt files and remove special characters and write file as utf-8 encoded\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "# Path to the directory containing the text files\n",
    "directory = \"./txt/\"\n",
    "\n",
    "# Iterate through the directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if the file is a text file\n",
    "    if filename.endswith(\".txt\"):\n",
    "        # Open the file in read mode and read its contents\n",
    "        with open(os.path.join(directory, filename), 'r', encoding=\"utf-8\") as f:\n",
    "            contents = f.read()\n",
    "            # Convert the encoding to utf-8\n",
    "            contents = contents.replace('\\n', ' ').replace('â€', '').replace('Â', '').replace('©', '').replace('*', '').replace('•', '').replace('*', '').replace('“', '').replace('”', '').replace('\f', '').replace(\"♦\", '')\n",
    "            # Open the file in write mode and write the utf-8 encoded contents\n",
    "            with codecs.open(os.path.join(directory, filename), 'w', encoding='utf-8') as f:\n",
    "                f.write(contents)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and read all the txt files and put them into chunks in a dataframe, this takes the contents of\n",
    "# the file and splits based on the text splitter.  this needs to be split because of the embeddings\n",
    "# columns will be title, tokens, content, summary, source\n",
    "\n",
    "path = \"./txt/\"\n",
    "chunk = {}\n",
    "txt = []\n",
    "\n",
    "for txt_file in os.listdir(path):\n",
    "    if txt_file.endswith(\".txt\"):\n",
    "        with open(os.path.join(path, txt_file), \"r\", encoding=\"UTF-8\") as f:\n",
    "            text = f.read()\n",
    "            texts = text_splitter.create_documents([text])\n",
    "            for i in texts:\n",
    "                chunk = {\n",
    "                        \"title\": txt_file[:-4],  # remove the .txt extension\n",
    "                        \"tokens\": num_tokens_from_string(i.page_content, \"cl100k_base\"),\n",
    "                        \"content\": i.page_content,\n",
    "                        \"source\": txt_file,\n",
    "                        }\n",
    "                txt.append(chunk)\n",
    "\n",
    "df = pd.DataFrame(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tokens</th>\n",
       "      <th>content</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Merck</td>\n",
       "      <td>7649</td>\n",
       "      <td>9-398-033 R E V :   O C T O B E R   1 7 ,   2 ...</td>\n",
       "      <td>Merck.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Merck</td>\n",
       "      <td>7510</td>\n",
       "      <td>went through a  similar  360-degree  process, ...</td>\n",
       "      <td>Merck.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Merck</td>\n",
       "      <td>263</td>\n",
       "      <td>Company document.   16   This document is auth...</td>\n",
       "      <td>Merck.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   title  tokens                                            content     source\n",
       "0  Merck    7649  9-398-033 R E V :   O C T O B E R   1 7 ,   2 ...  Merck.txt\n",
       "1  Merck    7510  went through a  similar  360-degree  process, ...  Merck.txt\n",
       "2  Merck     263  Company document.   16   This document is auth...  Merck.txt"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
