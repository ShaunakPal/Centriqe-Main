{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import textwrap\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "import openai\n",
    "\n",
    "#load our environment\n",
    "load_dotenv()\n",
    "\n",
    "ENV_NAM = os.getenv(\"ENV_NAME\")\n",
    "\n",
    "#configure Azure OpenAI Settings\n",
    "#os.environ[\"OPENAI_API_TYPE\"] = os.getenv\"azure\"\n",
    "#os.environ[\"OPENAI_API_BASE\"] = os.getenv\"YOUR_OPENAI_ENDPOINT\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "#os.environ[\"OPENAI_API_VERSION\"] = os.getenv\"2023-05-15\"\n",
    "model: str = \"text-embedding-ada-002\"\n",
    "vector_store_address: str = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "vector_store_key: str = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    ScoringProfile,\n",
    "    TextWeights,\n",
    ")\n",
    "\n",
    "embeddings: OpenAIEmbeddings = OpenAIEmbeddings(deployment=model, chunk_size=1)\n",
    "embedding_function = embeddings.embed_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_text(text: str, width: int = 120) -> str:\n",
    "    \"\"\"Wrap text to a specified width.\"\"\"\n",
    "    return '\\n'.join(textwrap.wrap(text, width))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str):\n",
    "\n",
    "    index_name: str = \"centriqe-vector-demo\"\n",
    "\n",
    "    vector_store: AzureSearch = AzureSearch(\n",
    "        azure_search_endpoint=vector_store_address,\n",
    "        azure_search_key=vector_store_key,\n",
    "        index_name=index_name,\n",
    "        embedding_function=embedding_function,\n",
    "    )\n",
    "\n",
    "    # Perform a similarity search\n",
    "    docs = vector_store.similarity_search(\n",
    "        query=query,\n",
    "        k=3,\n",
    "        search_type=\"similarity\",\n",
    "    )\n",
    "    \n",
    "    return docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample query to set the context\n",
    "query=input()\n",
    "context=search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('context.txt', 'a') as fp:\n",
    "    fp.write(str(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = f'You are a simple QA chatbot.  Use the following context to provde responses to questions: \\n\\n CONTEXT: \\n\\n{context}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('system.txt', 'a') as fp:\n",
    "    fp.write(str(system))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=query\n",
    "messages=[\n",
    "        {\"role\": \"system\", \"content\": f'{system}'},\n",
    "        {\"role\": \"user\", \"content\": f'{query}'}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('messages.json', 'a') as fp:\n",
    "    fp.write(json.dumps(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-16k\",\n",
    "    messages=messages,\n",
    "    temperature=1, #gives more idea generation and a less consistent story\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosquera estimated that it would take a minimum of three years to change the culture and create a new system at MSD.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create variables to collect the stream of chunks\n",
    "collected_messages = []\n",
    "# iterate through the stream of events\n",
    "for chunk in response:\n",
    "    chunk_message = chunk['choices'][0]['delta']  # extract the message\n",
    "    collected_messages.append(chunk_message)  # save the message\n",
    "    #print(f\"Message received: {chunk_message}\")  # print the delay and text\n",
    "\n",
    "full_reply_content = ''.join([m.get('content', '') for m in collected_messages])\n",
    "print(wrap_text(full_reply_content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
